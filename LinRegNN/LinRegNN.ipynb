{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression as a simple neural network\n",
    "\n",
    "Suppose we have a bivariate data set $x$ and $y$ and we wish to fit a linear model, $\\hat{y} = wx + b$, to it. The classical approach is to use least squares regression: a model which minimises the squares of the residuals between the measured value and the predicted value.\n",
    "\n",
    "This can also be achieved through the use of a simple neural network consisting of just an input layer of one neuron and an output layer of one neuron. There is one edge connecting them which scalar multiplies the input vector by a __weight__ $w$ and adds a __bias__ $b$ to each component of the vector.\n",
    "\n",
    "Our __training data__ for our neural network will be a randomly generated set of $n$ scores, made to be appropximately linear.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(1) # deterministic random seed\n",
    "n = 100 # number of scores in our data set\n",
    "\n",
    "x = np.random.uniform(low = 1, high = 100, size = n).reshape(n,1) # create random array for x values\n",
    "\n",
    "w = 2 * np.random.random() + 1 # random gradient between 1 and 3\n",
    "b = 10 * np.random.random() - 5 # random y-intercept between -5 and 5\n",
    "s = np.random.uniform(low = -10, high = 10, size = n).reshape(n,1) # create some variation around the line\n",
    "\n",
    "y = w * x + b + s\n",
    "\n",
    "plt.scatter(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classical linear regression\n",
    "Before we construct a neural network to find the linear model, we will use the classical linear regression model. Recall the gradient of the linear model is given by\n",
    "\\\\[ w = \\frac{\\sum_{i=0}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=0}^n (x_i - \\bar{x})^2}. \\\\]\n",
    "The $y$-intercept is then given by\n",
    "\\\\[ b = \\bar{y} - w \\bar{x}. \\\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w =  1.6621957496645583\n",
      "b =  -0.33931172631471895\n"
     ]
    }
   ],
   "source": [
    "w_calc = np.sum((x - x.mean())*(y - y.mean())) / np.sum((x - x.mean())**2)\n",
    "b_calc = y.mean() - w_calc * x.mean()\n",
    "print(\"w = \", w_calc) \n",
    "print(\"b = \", b_calc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the network\n",
    "We begin by setting $w$ and $b$ to zero. We then update these values through a loop which looks at the difference between the predicted values generated by $\\hat{y} = wx + b$ and the training data $y$ values. Obviously, this first model will be a terrible fit. To create a better fit we use an error function and minimise the error using a technique called __gradient descent__.\n",
    "\n",
    "Our error function of choice is the half mean squared error:\n",
    "\\\\[ J := \\frac{1}{2n} \\sum_{i=1}^n (\\hat{y}_i - y_i)^2. \\\\]\n",
    "To choose the direction in which we should move we use the negative of the __gradient__:\n",
    "\\\\[ \\nabla J = \\left[ \\begin{array}{c} \\frac{\\partial J}{\\partial w} \\\\ \\frac{\\partial J}{\\partial b} \\end{array} \\right]. \\\\]\n",
    "This is because the gradient gives us the direction in which we should travel to increase the value of a multivariable function most rapidly. The negative then gives us the direction to _decrease_ the value of a function most rapidly.\n",
    "\n",
    "For the mean squared error function, the gradient is found by substituting the equation for $\\hat{y}$:\n",
    "\\\\[ J = \\frac{1}{n} \\sum_{i=1}^n (wx_i + b - y_i)^2 \\\\]\n",
    "The gradient is then given by the partial derivatives:\n",
    "\\\\[ \\frac{\\partial J}{\\partial w} = \\frac{2}{n} \\sum_{i=1}^n x_i (w x_i + b - y_i) = \\frac{2}{n} \\sum_{i=1}^n x_i (\\hat{y} - y_i), \\\\]\n",
    "and\n",
    "\\\\[ \\frac{\\partial J}{\\partial b} = \\frac{2}{n} \\sum_{i=1}^n (w x_i + b - y_i) = \\frac{2}{n} \\sum_{i=1}^n (\\hat{y} - y_i). \\\\]\n",
    "\n",
    "To descend down the gradient carefully, we choose a parameter $\\eta$, sufficiently small, to move in the correct direction, but not so rapidly that we overshoot the minimum of the error function. The parameter $\\eta$ is known as the __learning rate__.\n",
    "\n",
    "Gradient descent is then a loop for a given number of __epochs__ (iterations), where in each epoch we update the weight and bias as follows:\n",
    "\\\\[ w_k = w_{k-1} - \\eta \\times \\frac{2}{n} \\sum_{i=1}^n x_i (\\hat{y} - y_i), \\\\]\n",
    "and\n",
    "\\\\[ b_k = b_{k-1} - \\eta \\times \\frac{2}{n} \\sum_{i=1}^n (\\hat{y} - y_i). \\\\]\n",
    "\n",
    "This is all achieved in the Python class below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.66179025] [-0.3123875]\n"
     ]
    }
   ],
   "source": [
    "eta = 0.0001\n",
    "epochs = 50000\n",
    "\n",
    "# initialise our weight and bias vectors with zeros\n",
    "w = np.zeros((n,1))\n",
    "b = np.zeros((n,1))\n",
    "\n",
    "for i in range(0,epochs):\n",
    "    y_predicted = w * x + b\n",
    "        \n",
    "    errors = y_predicted - y\n",
    "\n",
    "    w = w - eta * 2 * np.mean(errors * x)\n",
    "    b = b - eta * 2 * np.mean(errors)\n",
    "\n",
    "print(w[0], b[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
