{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression as a simple neural network\n",
    "\n",
    "Suppose we have a bivariate data set $x$ and $y$ and we wish to fit a linear model, $\\hat{y} = wx + b$, to it. The classical approach is to use least squares regression: a model which minimises the squares of the residuals between the measured value and the predicted value.\n",
    "\n",
    "This can also be achieved through the use of a simple neural network consisting of just an input layer of one neuron and an output layer of one neuron. There is one edge connecting them which scalar multiplies the input vector by a __weight__ $w$ and adds a __bias__ $b$ to each component of the vector.\n",
    "\n",
    "The data we're using consists of a training set: a randomly generated set of 700 pairs of points for which a linear model is suitable, and a testing set: a similarly randomly generated set of 300 pairs of points.\n",
    "\n",
    "Summary statistics and statter plots of both sets of data are below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>700.000000</td>\n",
       "      <td>700.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>49.992857</td>\n",
       "      <td>49.911608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>28.939406</td>\n",
       "      <td>29.097996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.839981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>24.985830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>48.936330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>74.880631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>108.871618</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                x           y\n",
       "count  700.000000  700.000000\n",
       "mean    49.992857   49.911608\n",
       "std     28.939406   29.097996\n",
       "min      0.000000   -3.839981\n",
       "25%     25.000000   24.985830\n",
       "50%     49.000000   48.936330\n",
       "75%     75.000000   74.880631\n",
       "max    100.000000  108.871618"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv(\"LinRegTrain.csv\")\n",
    "df_test = pd.read_csv(\"LinRegTest.csv\")\n",
    "\n",
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>300.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>50.936667</td>\n",
       "      <td>51.205051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>28.504286</td>\n",
       "      <td>29.071481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.467884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>25.676502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>52.170557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>74.303007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>105.591837</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                x           y\n",
       "count  300.000000  300.000000\n",
       "mean    50.936667   51.205051\n",
       "std     28.504286   29.071481\n",
       "min      0.000000   -3.467884\n",
       "25%     27.000000   25.676502\n",
       "50%     53.000000   52.170557\n",
       "75%     73.000000   74.303007\n",
       "max    100.000000  105.591837"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(df_train['x'], df_train['y'])\n",
    "plt.title(\"Training data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3df5Acd3nn8fezo5E9MqCRsCB4bVmGc8lnx2UJNlgXcTksKBR+2XuAYwyuODnnfH8kFzCUwsIRbCinUCKITeooUo4hMUfOCGTX2sFcBLHMcdHFCiuvjDG2DgNG1srgJdYqYI2t0e5zf0zPqne2e6ZnpmdnpufzqtranZ6e7h6P9ex3n36+z9fcHRERyZahbl+AiIikT8FdRCSDFNxFRDJIwV1EJIMU3EVEMkjBXUQkgxTcRVpkZj81s9d1+zpEoii4S18xs1+GvubMrBR6/N42jvugmV2T5rWGjn26mbmZnd2J44tEWdbtCxBphru/qPqzmT0J/J67/0P3rkikN2nkLpliZjkz+2Mz+5GZ/dzM/tbMisFzZ5jZl83sWTObMbN9ZrbKzD4N/Bpwe/AXwKdjjn2dmR0ys2kz21bz3ObgeMfM7IiZ3WJm1cHTt4PvB4Pjj5rZGjP7X8GxnjWze8zsFZ367yKDR8FdsmYb8CbgdcDZQBm4JXju96j8tToMnAn8AXDC3T8IfIfKXwEvCh4vYGYbgFuBq4LjrguOUVUOjrca+PfA24PzAfxG8H19cPxxKv/2/hJYC5wXPH8LIilRcJes+S/AmLsfcffngY8DV5mZUQnAa4BXuftJd/+Ouz+X8Li/Bdzl7v/k7i8AHyH078fd/zk43qy7/xC4HfgPcQdz95+5+z3uXnL3Y8An6+0v0izl3CUzggB+DvB1Mwt3xBsCXgp8HvgVYJeZvQj4IvDH7j6b4PBnAU9VH7j7MTM7Fjr3hcCngVcDBSr/tvbWudYXA58B3ggUg82FBNchkohG7pIZXmlxOgVscfdi6Ot0d/+5u7/g7h9z9wuopEquBN5dfXmDwz9N5RcHAGa2ElgZev6vgIeo/FXwEuATgNU59hiV9M6vBfu/KbS/SNsU3CVr/hLYbmbnAJjZy8zs7cHPbzSzC81sCPhX4CRQHbX/DHhlneN+BXiHmV1qZqcBNwNzoedfDBxz91+a2UXAf64+EaRxjtUc/8XAcWDGzM4EPtryOxaJoOAuWfNnwD8Ae8zsF8D/pZIqgcqN1HuAXwDfA75OJWhD5Wbmb5vZUTP7s9qDuvsk8EFgF3AYOAT8PLTLDcDvmdkvgc8CO2sO8THgq0GVzuXAp6jckP0X4B+DaxFJjWmxDhGR7NHIXUQkgxTcRUQySMFdRCSDFNxFRDKoJyYxnXnmmb5u3bpuX4aISF/Zv3//z919TdRzPRHc161bx8TERLcvQ0Skr5jZT+KeU1pGRCSDFNxFRDJIwV1EJIMU3EVEMkjBXUQkg3qiWkZEZNCMT06xY/dBjsyUOKtYYNvW9YxuHE7t+AruIiJLbHxyim1ffZjyXKVx49RMiW1ffRggtQCvtIyIyBK76d5H5wN7VXnOueneR1M7h4K7iMgSmymVm9reCgV3EZEMUnAXEVliZyzPNbW9FQruIiJLLJ+LDr1x21uh4C4issSOxeTW47a3QqWQIiIpa1TDflaxwNRMadHrzioWUrsGjdxFRFI0PjnFh+9+hKmZEk6lhv3Ddz/C+OTU/D7btq6nkF+YXy/kc2zbuj6162gY3M3sC2b2jJl9L7RttZl908x+EHxfFWw3M/sLM3vCzL5rZq9O7UpFRPrAjt0HKZVnF2wrlWfZsfvg/OPRjcN88h0XM1wsYMBwscAn33Hxks9Q/RvgvwNfDG0bA+539+1mNhY8/hDwZuD84OtS4HPBdxGRvtRsm4AjEemW6vbwsVYW8ph16qoTjNzd/dvAszWbrwDuCH6+AxgNbf+iVzwIFM3sFWldrIjIUkqSYqkVlzdfWcgvONZMqczR4+XEx21Wqzn3l7v70wDB95cF24eBp0L7HQ62LWJm15vZhJlNTE9Pt3gZIiKdkyTFUisun27GomM1c9xmpX1DNeqPDI/Yhrvf5u4j7j6yZk3k+q4iIh01PjnF5u17OG/sPjZv37No5FwvxRInLp8+c7xxmWO94zar1VLIn5nZK9z96SDt8kyw/TBwTmi/s4Ej7VygiEgnVFMu1dF0NTUCpzoz1itZjMqfzxwvc1axwLqXFvjpsedx4KfHnmfiJ89SXJHnaIMA3wulkPcC1wY/XwvcE9r+20HVzCbgWDV9IyLSS5KkXKJSLEblF8H7dx6IzZ/v/eGzzHolaTHrzpcePMS/NpiglHYpZMORu5ndCbweONPMDgM3AtuBr5jZdcAh4Mpg968DbwGeAI4Dv5valYqIpChJyqU6gt+x++D8CD4yz5zAbJ0XDndjsQ53vzrmqTdE7OvA77d7USIinZZ0lujoxmFGNw6z8RPfaJhWadXesS2pH1PtB0RkIG3bun7BakhVUzMlNnz8G/M59OKKPO7p9loPKxbyHTmugruIDK6YSUThQN6p0TpAfsi46fKLOnJsBXcRGUg7dh+kXC8R3mGdyLOHKbiLyEBKs6a8WcPFQkfy7GHqCikiAynNmvI4t161oePdH+No5C4iA6U6+SiqUqZZRnxp5KoV+QWllEkbj6VFwV1EBsb45BTbdj3cMNduwIrlOZ47Ed8LppozBxYdM58zbnx75UZptZRyqSm4i8jA+PjfPZroJqpD3cBuLK5N78bovB4FdxHJpKg+7GmVNdb2lumVgB6m4C4imVOvKVi7Cvkcl12wpmHTsW5TtYyIZE5cU7B2Vdv3PvD4dNN93peagruIZE4nathXrcizd2wLoxuHW+rzvtSUlhGRntVqXjuuKVi90sVGPPTCpE3HukkjdxHpSa2sX1p12QXRq7u102zgWKjfTNxSeksxOSkpBXcR6UmtrF9a9cDj6a/LHB6Vxy2l1ys3U0FpGRHpUe3ktdPOfUeNyrs1OSkpjdxFpCfF5a+T5LXTzH334qg8CQV3EelJ7eS1o15bj1Fp8hVOs9x61Qae3P7W+QqZfqO0jIj0pNGNw0z85Fnu3PcUs+7kzHjnayqpkEZVNFFrn9ZzVrHQ82mWZim4i0hPGp+c4q79U8wGNYiz7ty1v1Ipc9f+qYazQ6vBulGzsF6rckmLgruI9KS4apnqSL52+47dBxcE9/DofmUhv2hN1GOlck/2hEmLgruI9KS4ipfawB61f21vmZlSmUI+xy1XbchkII+i4C4iPSl2lqktnC1aVVyRr7sQR9ToPstULSMiPSmuWqawLDpsPV+enZ/RGqeXer90moK7iPSkuFmgpfJc5P6l8lzDzo+91Pul05SWEZElk8YCF3HpmkayWhUTR8FdRDoqnAcPd2VstMBF3IIb73zN8IJSSKgE7tPzQ7ErLQ1nuComTltpGTO7wcweNbPvmdmdZna6mZ1nZvvM7AdmttPMlqd1sSLSX8KdHWFxV8Z6jcDiSiEfeHw6Ml1z49sviszR33rVhr6dZdqOlkfuZjYM/CFwobuXzOwrwLuBtwC3uPuXzewvgeuAz6VytSLSV6ICdK1mG4QdmSnVnU3ay+uaLqV20zLLgIKZlYEVwNPAFuA9wfN3ADeh4C4yUOqVJNaKW2y6lQUxstZCoB0tB3d3nzKzTwGHgBLwDWA/MOPuJ4PdDgP6Ly0yQGpz5fVUF5sOtweYminx/p0HgMUrJw3aTdF2tJOWWQVcAZwHzABfBd4csWvkdDIzux64HmDt2rWtXoaIdFizFS5JUjFQCdyl8ixfevBQ7D7OqQA/iDdF29FOWuaNwI/dfRrAzO4Gfh0omtmyYPR+NnAk6sXufhtwG8DIyEg7q1+JSIfEVaxAdIVLdZ8kkv6jrwb2vWNbEr5CoL1qmUPAJjNbYWYGvAH4PvAA8K5gn2uBe9q7RBHplPHJKTZv38N5Y/exefueReuTtrLUXc4s9escpJmlaWk5uLv7PmAX8BDwSHCs24APAR8wsyeAlwKfT+E6RSRlSRagbmWpu7jGXu0YpJmlaWmrWsbdbwRurNn8I+C17RxXRDqv3qi8mnKJq1hxYPP2PVx2wRoeeHx6QT5+uMUZpHF0E7U16i0jMqCSjMrrLVc3NVPiSw8eWjTyv+yCNeSHFqZm8kPGNZvWJlr6rljIs2pFfsEEJd1EbZ7aD4gMqCR15LVL3TVSKs9y33efrpS4hBmMnLuakXNXz1ferFie4/iJWZxKnv7qS8/h5tGL23xXUqXgLjKgtm1dz7avPkx57lTQzg/ZghRI7VJ3SUT1dynPOjt2H1xw7OKK5fzJf1RpY6couIsMsogRdljSmvUkqmmbRmWVaXSOFOXcRQbWjt0HFy0aXR1hV6VZgpgza1hWmaSCR5JRcBcZUEluqKZVgljI5xKtfdpKXb1EU3AXGVBxgTu8vV61TFLVipfhBOdrpa5eoim4iwyouDVKwzc9w0vdtaraSz3J+ZL8wpFkFNxFBtToxmHe+Zrh+XYBOTPe+ZrFLXNHNw6zd2zLonuvSQzXlFVGLbIRPl+SXwCSjKplRAZM3LJ3s+7ctX+KkXNXA4sXvVhZyDNTil7GLkpUUG7Ub736nKpl2mfegT4QzRoZGfGJiYluX4ZI5iXptV4s5Hnh5NyiNUrn3Hnh5Nyi/asj+uKKPO5wrFRWUF4iZrbf3UeintPIXSSjourFk9StR43O673GgSe3v7Xdy5WUKbiLZEhcyqV2ApFkn4K7SEbUplxqE66dCuzFQr4jx5X2KLiL9LmPjj+SuLFX2vJDxk2XX7Tk55XGFNxF+thHxx+puwZpJ2lN096m4C7Sx+7c91TXzq01TXubJjGJ9LFupGKgM+ukSroU3EX6WLeC7NWXntOV80pyCu4ifSytIFvvl0QhP0R11bycVZbL04pJvU85d5E+dvPoxfx4+pfs/eGzbR1n0ytX8dChY4tmpWr90v6lkbtIHxufnOKhQ8cS7VsvgfPkv5QaNvWS/qKRu0iPiVtmLry92sclaSOvQj5XdxLTkZlSw6Ze0l8U3EV6SO0s02rbgImfPMtd+6fmt0ctQh0nanm7WuqXnj0K7iI9INwTplapPNvyDNRGI/bqPuqXnj3KuYt0WXhR6DitBPZGy9uF91E6Jns0chfpsLgcelWSNrxDBnNNxvfwDNLajpCqhMk+BXeRDorLocOpVYfqjdirck0G93DdulY3GkxtBXczKwK3A79KpcPofwIOAjuBdcCTwG+5+9G2rlKkR4VH5SsLecxg5ni57uIYpfIsO3YfnA+uObOGaZfy4gWQ6qqd3KRKmMHTbs79M8Dfu/sFwCXAY8AYcL+7nw/cHzwWyZxwrtyplCUePV7GOTVCjxuVHwltT7M/jGaQSlXLI3czewnwG8DvALj7CeCEmV0BvD7Y7Q7gW8CH2rlIkV7UKFdeKs/GjsrDpYerVuSbKm2Mks8ZO951iUbnMq+dkfsrgWngr81s0sxuN7MzgJe7+9MAwfeXRb3YzK43swkzm5ienm7jMkS640iCXPmsO4V8bsG22tLD59NYIan769xLj2knuC8DXg18zt03As/RRArG3W9z9xF3H1mzZk0blyHSHUkm/uTMYqf1j09OsXn7HkrNJtQjlOecHbsPtn0cyY52gvth4LC77wse76IS7H9mZq8ACL4/094livSmbVvXLxqV14rLp49PTrFt18OJKmWSSvKXhAyOlnPu7v5TM3vKzNa7+0HgDcD3g69rge3B93tSuVKRLqpXqx43sxQqi0dHlUIaTnk23VyKWghIWLt17v8V+FszWw78CPhdKn8NfMXMrgMOAVe2eQ6RrmpUq15NsdROFDKiG3s1mrBUdc2mtQCJWg+ohYDUMu/SMl1hIyMjPjEx0e3LEIm0efueyJH5cLGwYBZovf4wrYg7flxNvSplBo+Z7Xf3kajnNENVpIG4XHbt9uoofuMnvtF2aWO944skocZhIg3E5bLjtqcR2OsdXyQJjdxFGti2dX1kPn1qpsTGT3wDdzhWOpUeSYNy6NIuBXeRBmqrYoxTc4bCo/SpmRI37DzQ9vmGlUOXFCgtI5LA6MZh9o5tYbhYqDsZtN3yhGs2rWXv2BYFdmmbRu4yEJJUmjTquw6dnyj0wONqxSHpUHCXzKutQQ/Xnk/NlNi26+FFa5RG9V0HWFnIJ16UuhWaZSppUVpGMq9R98byrPM/9x2K7Lv+/p0HeNWHv85HxyuBPrQGRkeoQkbSopG7ZF6S0XC9VY5m3fnSg4eASiqnU1QhI2nSyF0yL63R8J37nurYyFoLVUvaFNwl8y67IJ2W0rPuiTpBNqvaZkCBXdKktIz0vUZVLmlVoOTMEnWCbIZSMdIpCu7S0xoF7kYdGyG9CpTqotPVHi/njd0XW9eeGzJm6yXy0WQl6SylZaRn1S5AXQ3c45NT8/tEVcKUyrMLViVaWchHHj+u8GW4WOCaTWvJBaUxcYtOx+Xfh4sFPn3lJQzXyc8rFSOdppG79Jx6rXOrgbvRqDy8Pa58ccXyHCdOzlEOjbDzQzY/mq4N5rWies5U0yz1+rwrFSNLQcFdekpUMKwVDtxnFQuRvwTCo+q4Lo3PnYg4R8wvgkYrMcWljZLsI9IJCu7SUxpNOIKFgbtex8bN2/ewbet6hqx+HXtYedYX/GUAyVZiqkd92KUblHOXntLo5mc4cI9PTjG6cZhPvuPi+fx2uGNjNQgnDexx15Akry/SaxTcpafUmyQUFbirAT6uY2PS9UrrXUPSlZhEeomCu/SUqElChXyOVSvykYE7PHpOI9hG3exsdiUmkV6g4C49JZxmMU5Ny4/r6XJkpsT45BSbt+9puZd69R5qXAuAuF84qniRXmbu7S4v0L6RkRGfmJjo9mVID9u8fU9kVUyxkOeFk3MtpV8g+USiJL3eRZaame1395Go51QtI30hrqbcrPm8eiszQ1XxIv1GwV36Qm29eHU1pbga9jjVmaEiWafgLkuqnfRGeNbnB75yoG6JoxksGzLKs6HZpzlTnlwGhoK7LJkkTb6iXlP7y+Ajd3+3Ye26O4tXq+7+7SWRJaNqGVkyzU4Gimscdrw81/BcObMFPWMAynOuiUcyMNoO7maWM7NJM/ta8Pg8M9tnZj8ws51mtrz9y5QsiKtDn5opLVintCrul0EjhXyO2ZgqME08kkGRxsj9fcBjocd/Ctzi7ucDR4HrUjiHZEBc6104tU5pOMC3Eoirtepx7XY18UgGRVvB3czOBt4K3B48NmALsCvY5Q5gtJ1zSHbEtd4Nu3PfU/M/NxuIr9m0dr5HuiYeyaBrd+R+K/BHQDUJ+lJgxt1PBo8PA5F3yszsejObMLOJ6el0lkGT3hY3yzRs1n2+Kdi2revJ5xL8RgCGDEbOXT3/OG6mq2rVZVC0XC1jZm8DnnH3/Wb2+urmiF0jk5/ufhtwG1RmqLZ6HdJdzZQ2rizkmSk1DvBTMyVu2HmgqeKWOWdRq15NPJJB1k4p5GbgcjN7C3A68BIqI/mimS0LRu9nA0fav0zpRc2WNiZJy1S18tu+2mdGbQJE2kjLuPuH3f1sd18HvBvY4+7vBR4A3hXsdi1wT9tXKT2p2dLGJGmZdqws5BuuuSoyKDpR5/4h4ANm9gSVHPznO3AO6QH1ShurefOwTlaqxPWZ0aIaMqhSCe7u/i13f1vw84/c/bXu/m/c/Up3fyGNc0jvqVfaGDVqjqpgaUdtq956bYFFBo3aD0jLGuXQw6PmcMOv0/NDTTf8gkp7X7NKeicqn75j98GGi2WLDAoFd2lZkhz61EyJ9+88cOo1pXLi8sawJN0c49oCq7ZdBpF6y0jLWh0Rl2e9qcqZpAFate0ip2jkLi3btnU923Y9vKCtblL1FgA7Y3mO4ydmcSoNwN75muT16qptF6lQcJcFmq4TT3n62aoVeZ4vz80fdtadu/ZPMXLuagVtkSYoLSPz4lrsxtWJ79h9cFFb3XYdPV5WOaNIChTcZV7SSUnjk1OxC1ZX1Uup54eMVSviyyijqJxRpDkK7jIvLoCGt49PTrFt18N1A/twscAtV22Yv7G5akW+UsYYPLfjyku48e0XRXZtLMbUzqucUaQ5yrkPiLhceng7RmQOPTxZ6eN/92jdG6hGJZ2zY/fBRH1daq8JUDmjSAoU3AdAXIOviZ88y137p04F0piYHS5brDf5KPy7Icn6qPUqW9T8S6Q9Cu4DIC6Xfue+p2KXowtL2vCr9kjVfH2zgVnljCLtU859AMTl0pMEdliYlonLiTd7bhHpLAX3ARB3MzKXcJpoeLebLr+I/FDy6aW6ESrSHQruAyBuPdGrLz0nUZfGcFpmdOMwO668JHYB6tpz6EaoSHcouA+AuJ4rN49evGB73Hi8trXv6MZh9o5tqRvg1ddFpLt0Q3VAtHOTsjw7F7k9rgujgrpI9ym4D7DaEsk4z52YZcPHvxHbS11liyK9R8E9A8ITkVaGFrQorsjjDsdK5ciJS/VmmdaaKZ3Ku9fWsCuYi/QeBfc+Vzv6Dgfh8ISj2IlLLWq1hl1EloaCe5+LmqAUp5mJS0kkrWFvuo2wiLRN1TJ9rtlJQo0CezMdGx3YvH1PbEtgaL6NsIikQ8G9zzU7SajexKWcGVe99hwmP/Ymbr1qQ6Ia+CQ939WfXWTpKbj3qXBP9aTzRRtNXKquejQ+ObWoNr5YyMeO6OsF6yRthEUkfcq596Ham6hObLfeecOhXPfIuatjq2XCN0qjKmHOG7sv8jxxwfqsYiHyPGpLINJZGrn3kepo/f07DyxKdTS6Rbp3bMt8oK7OMI0b8dcbVccF5bjtca0P1JZApLM0cu+iJFUk4Zr0RqPzOMPFQuS5iivykf3Zi3VuqMbNSo0L1proJNIdCu5dEreABpwKiFHpl3qKhTwvnJxbFHgvu2BNzLmij1ivoKaVYK2JTiJLr+XgbmbnAF8EfgWYA25z98+Y2WpgJ7AOeBL4LXc/2v6lZku9KpJwAE1aw17I57jp8ovmXxcOvHHninOsVH9xDgVrkd7Xzsj9JPBBd3/IzF4M7DezbwK/A9zv7tvNbAwYAz7U/qVmS5IqkqQVJTkz3vma4QU59bAbdh5o6tp0s1Ok/7V8Q9Xdn3b3h4KffwE8BgwDVwB3BLvdAYy2e5FZlOTGZL3cd9isO1968BDrxu6LnFQUd65VK/K62SmSUalUy5jZOmAjsA94ubs/DZVfAMDLYl5zvZlNmNnE9PR0GpfRV5JUkbTSJWBqpsQNOw8sCPRx57rx7RdF9nlXykWk/5m32WfEzF4E/G/gT9z9bjObcfdi6Pmj7r6q3jFGRkZ8YmKirevoR3HVMq10bYxT7a8OqlgRyRoz2+/uI5HPtRPczSwPfA3Y7e5/Hmw7CLze3Z82s1cA33L3un/nD2pwj5K0x3ozhosF9o5tSe14ItIb6gX3dqplDPg88Fg1sAfuBa4Ftgff72n1HP0sPCqP66v+3r/6J/b+8Nn512x+1Wqe/JdSqoEdNNVfZBC1PHI3s9cB/wd4hEopJMBHqOTdvwKsBQ4BV7r7s5EHCWRt5N5o9F3I5zh71en84JnnluR6NHIXyaaOjNzd/R+JX1P5Da0eNwsa1aeXyrMdC+y1s1hV/SIymNRbpgPaTYMkabUb55arNqj6RUTUfqAT4johJnV6fojTlg0tWDIviZyZZo+KCKCRe0ds27qefC5pl/XFjh4v88LJOc5YHj2CP21Z9Md29aXntHxOEckWBfdOaXOZ0lJ5ludOROftT5yc45pNa+dXVcqZcc2mtdw8enF7JxWRzFBapgN27D5IeS6dRaijnFUscPPoxQrmIhJLwT1Fac4shUrly+n5XOLe6SIiVQruJFs0I8kxPrDzwHzBfxoc+OQ7LlbbABFp2sAH9ySLZiTx4bu/m2pgh0opo6pfRKQVAx/ckyyaAY1H96VyuqFd6RcRacfAB/e4CUdTMyU2b98zH2DTGN0nNaz0i4i0aeCDe70JR9UgftqyoYaj+yGDNApkDNQHRkTaNvB17lELWYSVyrOxM0XDo/73XLo20fkaTW3SEncikoaBGbnX5swvu2ANDzw+zZGZEisLeU7PD3H0eHPT/c8qFpouf6w3uFeeXUTSMhDBPaoi5ksPHpp/fqZUppDPNZVaKeRzXHbBmtQW1lCeXUTSNBDBvVELXqCpAF0NxEmOG2YWvS6q+q2LSNoGIuee1oxROBWIRzcON9XaN58z3nvp2oaLYouIpGEggnu1wVYjxUK+7s3V2kCc9ObncLHAjnddws2jF/PJd1ysfusi0nEDkZaZTbCUYH7IuOnyiwDmb7yuLOQxg5nj5ciJS9u2rm+Yc68tbdSMUxFZCpkI7o1mj56xPBfbPndeMLgPB9/qcWdiqmiq+9WrllFpo4h0Q9+nZaqVMFMzJZxTE4/GJ6fm9zneKLAD5Vlnx+6DTR0XKgF+79gWbr1qg/LpItIz+j641+sNU5V04mj4BmmS44aNbhxWPl1Eekbfp2XiKlbC23NmifLu4RRKkuPWUj5dRHpF34/c43La4e1J1hZNWgmjHLqI9IO+D+5RvWFqA/XNoxez+VWrF+xz/svOmE+hFIP2AzfsPMDm7XsYn5xKdFwRkV7V98E9Sa57fHKKhw4dW/C6w0efZ9vW9dxy1QZeODnH0ePlBTdOAeXQRaRvmSfIRXfayMiIT0xMdOz4m7fviSxVHA5SLHHPqSWAiPQyM9vv7iNRz/X9DdUk6i3I0exrRET6QUfSMmb2m2Z20MyeMLOxTpwjbHxyis3b93De2H3zOfOwVm6C6sapiPSz1IO7meWAzwJvBi4ErjazC9M+T1WSyUaNFuSopRunItLvOpGWeS3whLv/CMDMvgxcAXw/zZPUWySjdgm8cJuAI8EvgTjqqy4iWdCJ4D4MPBV6fBi4NM0T1C6+EaU2Zx6eYFTvBqtuoopIFnQi5x7VX3fRYNnMrjezCTObmJ6ebuoESRbJcIjMv0Oy2ngRkX7WieB+GAhPCT0bOFK7k7vf5u4j7j6yZs2apk6QtJKlXrMv1bCLSJZ1Ii3zHeB8MzsPmALeDUNgmYAAAAS1SURBVLwnzROsLOSZKSVbzLo2/16lPjAikmWpB3d3P2lmfwDsBnLAF9z90TTPkXBhpXlpLrMnItIPOjKJyd2/Dny9E8cGOBqzeEacpMvsiYhkRV/2lmk2WCdp9ysikiV9GdybDdbDmm0qIgOmL4N7M8FaJY4iMoj6MrhH1alXEzWrVuQpFvIqcRSRgdaXXSFr2wmcpZYBIiIL9GVwB9Wpi4jU05dpGRERqU/BXUQkgxTcRUQySMFdRCSDFNxFRDLIvAem5pvZNPCTFl9+JvDzFC+nH+g9Dwa958HQzns+190je6b3RHBvh5lNuPtIt69jKek9Dwa958HQqfestIyISAYpuIuIZFAWgvtt3b6ALtB7Hgx6z4OhI++573PuIiKyWBZG7iIiUkPBXUQkg/o6uJvZb5rZQTN7wszGun09nWBm55jZA2b2mJk9ambvC7avNrNvmtkPgu+run2taTKznJlNmtnXgsfnmdm+4P3uNLPl3b7GNJlZ0cx2mdnjwWf97wbgM74h+H/6e2Z2p5mdnrXP2cy+YGbPmNn3QtsiP1er+Isgnn3XzF7dzrn7NribWQ74LPBm4ELgajO7sLtX1REngQ+6+78FNgG/H7zPMeB+dz8fuD94nCXvAx4LPf5T4Jbg/R4FruvKVXXOZ4C/d/cLgEuovPfMfsZmNgz8ITDi7r8K5IB3k73P+W+A36zZFve5vhk4P/i6HvhcOyfu2+AOvBZ4wt1/5O4ngC8DV3T5mlLn7k+7+0PBz7+g8o9+mMp7vSPY7Q5gtDtXmD4zOxt4K3B78NiALcCuYJesvd+XAL8BfB7A3U+4+wwZ/owDy4CCmS0DVgBPk7HP2d2/DTxbsznuc70C+KJXPAgUzewVrZ67n4P7MPBU6PHhYFtmmdk6YCOwD3i5uz8NlV8AwMu6d2WpuxX4I2AuePxSYMbdTwaPs/ZZvxKYBv46SEXdbmZnkOHP2N2ngE8Bh6gE9WPAfrL9OVfFfa6pxrR+Du4WsS2zdZ1m9iLgLuD97v6v3b6eTjGztwHPuPv+8OaIXbP0WS8DXg18zt03As+RoRRMlCDPfAVwHnAWcAaVtEStLH3OjaT6/3k/B/fDwDmhx2cDR7p0LR1lZnkqgf1v3f3uYPPPqn+yBd+f6db1pWwzcLmZPUkl1baFyki+GPz5Dtn7rA8Dh919X/B4F5Vgn9XPGOCNwI/dfdrdy8DdwK+T7c+5Ku5zTTWm9XNw/w5wfnB3fTmVmzH3dvmaUhfkmz8PPObufx566l7g2uDna4F7lvraOsHdP+zuZ7v7Oiqf6R53fy/wAPCuYLfMvF8Ad/8p8JSZrQ82vQH4Phn9jAOHgE1mtiL4f7z6njP7OYfEfa73Ar8dVM1sAo5V0zctcfe+/QLeAvw/4IfAf+v29XToPb6Oyp9m3wUOBF9voZKHvh/4QfB9dbevtQPv/fXA14KfXwn8M/AE8FXgtG5fX8rvdQMwEXzO48CqrH/GwMeBx4HvAf8DOC1rnzNwJ5V7CmUqI/Pr4j5XKmmZzwbx7BEqlUQtn1vtB0REMqif0zIiIhJDwV1EJIMU3EVEMkjBXUQkgxTcRUQySMFdRCSDFNxFRDLo/wNdNQTxdo1vYAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(df_test['x'], df_test['y'])\n",
    "plt.title(\"Test data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classical linear regression\n",
    "Before we construct a neural network to find the linear model, we will use the classical linear regression model. Recall the gradient of the linear model is given by\n",
    "\\\\[ w = \\frac{\\sum_{i=0}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=0}^n (x_i - \\bar{x})^2}. \\\\]\n",
    "The $y$-intercept is then given by\n",
    "\\\\[ b = \\bar{y} - m \\bar{x}. \\\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w =  1.0007782480808467\n",
      "b =  -0.12015553181318239\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# create vectors of the appropriate shape of training data values\n",
    "x = np.array(df_train['x']).reshape(-1,1)\n",
    "y = np.array(df_train['y']).reshape(-1,1)\n",
    "\n",
    "w_calc = np.sum((x - x.mean())*(y - y.mean())) / np.sum((x - x.mean())**2)\n",
    "b_calc = y.mean() - w_calc * x.mean()\n",
    "print(\"w = \", w_calc) \n",
    "print(\"b = \", b_calc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the network\n",
    "We create two 700 dimensional vectors $w$ and $b$ initialised with zeroes. We then update these values through a loop which looks at the difference between the predicted values generated by $\\hat{y} = wx + b$ and the training data $y$ values. Obviously, this first model will be a terrible fit. To create a better fit we use an error function and minimise the error using a technique called __gradient descent__.\n",
    "\n",
    "Our error function of choice is the mean squared error:\n",
    "\\\\[ J := 1/n \\sum_{i=1}^n (\\hat{y}_i - y_i)^2. \\\\]\n",
    "To choose the direction in which we should move we use the negative of the __gradient__:\n",
    "\\\\[ \\nabla J = \\left[ \\begin{array}{c} \\frac{\\partial J}{\\partial w} \\\\ \\frac{\\partial J}{\\partial b} \\end{array} \\right]. \\\\]\n",
    "This is because the gradient gives us the direction in which we should travel to increase the value of a multivariable function most rapidly. The negative then gives us the direction to _decrease_ the value of a function most rapidly.\n",
    "\n",
    "For the mean squared error function, the gradient is found by substituting the equation for $\\hat{y}$:\n",
    "\\\\[ J = \\sum_{i=1}^n (wx_i + b - y_i)^2 \\\\]\n",
    "The gradient is then given by the partial derivatives:\n",
    "\\\\[ \\frac{\\partial J}{\\partial w} = \\frac{2}{n} \\sum_{i=0}^n x_i (w x_i + b - y_i) = \\frac{2}{n} \\sum_{i=0}^n x_i (\\hat{y} - y_i), \\\\]\n",
    "and\n",
    "\\\\[ \\frac{\\partial J}{\\partial b} = \\frac{2}{n} \\sum_{i=0}^n (w x_i + b - y_i) = \\frac{2}{n} \\sum_{i=0}^n (\\hat{y} - y_i). \\\\]\n",
    "\n",
    "To descend down the gradient carefully, we choose a parameter $\\gamma$, sufficiently small, to move in the correct direction, but not so rapidly that we overshoot the minimum of the error function. The parameter $\\gamma$ is known as the __learning rate__.\n",
    "\n",
    "Gradient descent is then a loop for a given number of __epochs__ (iterations), where in each epoch we update the weight and bias as follows:\n",
    "\\\\[ w_k = w_{k-1} - \\gamma \\times \\frac{2}{n} \\sum_{i=0}^n x_i (\\hat{y} - y_i), \\\\]\n",
    "and\n",
    "\\\\[ b_k = b_{k-1} - \\gamma \\times \\frac{2}{n} \\sum_{i=0}^n (\\hat{y} - y_i).  \\\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w =  [6.66433512e-05] b =  [9.98232169e-07] MSE =  3336.6524577344967\n",
      "w =  [0.48632877] b =  [0.00728275] MSE =  884.2553137101332\n",
      "w =  [0.73582909] b =  [0.01101571] MSE =  238.61306099308564\n",
      "w =  [0.86384732] b =  [0.01292778] MSE =  68.63491527551669\n",
      "w =  [0.92953331] b =  [0.01390557] MSE =  23.884792338585093\n",
      "w =  [0.96323673] b =  [0.01440397] MSE =  12.103433407168508\n",
      "w =  [0.98052994] b =  [0.0146564] MSE =  9.001756354611075\n",
      "w =  [0.98940311] b =  [0.01478263] MSE =  8.185177933788166\n",
      "w =  [0.99395597] b =  [0.0148441] MSE =  7.970197109496857\n",
      "w =  [0.99629208] b =  [0.01487234] MSE =  7.9135988001584625\n",
      "w =  [0.99749079] b =  [0.01488354] MSE =  7.898697833366464\n",
      "w =  [0.99810589] b =  [0.01488598] MSE =  7.894774520136277\n",
      "w =  [0.99842155] b =  [0.01488394] MSE =  7.893741292254558\n",
      "w =  [0.99858356] b =  [0.0148796] MSE =  7.893468936771302\n",
      "w =  [0.99866674] b =  [0.01487408] MSE =  7.89339689609161\n",
      "w =  [0.99870947] b =  [0.01486795] MSE =  7.893377592296033\n",
      "w =  [0.99873144] b =  [0.01486151] MSE =  7.893372172566398\n",
      "w =  [0.99874276] b =  [0.0148549] MSE =  7.8933704081267075\n",
      "w =  [0.99874862] b =  [0.01484822] MSE =  7.8933696060485365\n",
      "w =  [0.99875168] b =  [0.0148415] MSE =  7.893369057364988\n",
      "w =  [0.9987533] b =  [0.01483475] MSE =  7.893368575426449\n",
      "w =  [0.99875418] b =  [0.014828] MSE =  7.893368111093695\n",
      "w =  [0.99875468] b =  [0.01482124] MSE =  7.893367651429846\n",
      "w =  [0.99875498] b =  [0.01481447] MSE =  7.893367193029008\n",
      "w =  [0.99875519] b =  [0.01480771] MSE =  7.893366734994509\n",
      "w =  [0.99875534] b =  [0.01480094] MSE =  7.8933662770902835\n",
      "w =  [0.99875547] b =  [0.01479418] MSE =  7.89336581925417\n",
      "w =  [0.99875559] b =  [0.01478741] MSE =  7.893365361469804\n",
      "w =  [0.9987557] b =  [0.01478065] MSE =  7.893364903732875\n",
      "w =  [0.9987558] b =  [0.01477389] MSE =  7.893364446042246\n",
      "w =  [0.99875591] b =  [0.01476712] MSE =  7.893363988397612\n",
      "w =  [0.99875601] b =  [0.01476036] MSE =  7.893363530798888\n",
      "w =  [0.99875611] b =  [0.01475359] MSE =  7.89336307324605\n",
      "w =  [0.99875621] b =  [0.01474683] MSE =  7.89336261573909\n",
      "w =  [0.99875631] b =  [0.01474007] MSE =  7.893362158277998\n",
      "w =  [0.99875641] b =  [0.01473331] MSE =  7.893361700862771\n",
      "w =  [0.99875652] b =  [0.01472654] MSE =  7.8933612434934055\n",
      "w =  [0.99875662] b =  [0.01471978] MSE =  7.893360786169897\n",
      "w =  [0.99875672] b =  [0.01471302] MSE =  7.8933603288922365\n",
      "w =  [0.99875682] b =  [0.01470626] MSE =  7.893359871660424\n",
      "w =  [0.99875692] b =  [0.0146995] MSE =  7.893359414474457\n",
      "w =  [0.99875702] b =  [0.01469274] MSE =  7.893358957334326\n",
      "w =  [0.99875712] b =  [0.01468598] MSE =  7.893358500240025\n",
      "w =  [0.99875723] b =  [0.01467922] MSE =  7.8933580431915535\n",
      "w =  [0.99875733] b =  [0.01467246] MSE =  7.893357586188906\n",
      "w =  [0.99875743] b =  [0.0146657] MSE =  7.893357129232078\n",
      "w =  [0.99875753] b =  [0.01465894] MSE =  7.893356672321064\n",
      "w =  [0.99875763] b =  [0.01465218] MSE =  7.89335621545586\n",
      "w =  [0.99875773] b =  [0.01464542] MSE =  7.893355758636463\n",
      "w =  [0.99875783] b =  [0.01463867] MSE =  7.893355301862866\n",
      "w =  [0.99875793] b =  [0.01463191] MSE =  7.893354845135065\n",
      "w =  [0.99875804] b =  [0.01462515] MSE =  7.893354388453052\n",
      "w =  [0.99875814] b =  [0.01461839] MSE =  7.89335393181683\n",
      "w =  [0.99875824] b =  [0.01461164] MSE =  7.893353475226391\n",
      "w =  [0.99875834] b =  [0.01460488] MSE =  7.893353018681728\n",
      "w =  [0.99875844] b =  [0.01459813] MSE =  7.893352562182837\n",
      "w =  [0.99875854] b =  [0.01459137] MSE =  7.893352105729716\n",
      "w =  [0.99875864] b =  [0.01458462] MSE =  7.89335164932236\n",
      "w =  [0.99875874] b =  [0.01457786] MSE =  7.89335119296076\n",
      "w =  [0.99875885] b =  [0.01457111] MSE =  7.893350736644921\n",
      "w =  [0.99875895] b =  [0.01456435] MSE =  7.893350280374826\n",
      "w =  [0.99875905] b =  [0.0145576] MSE =  7.893349824150481\n",
      "w =  [0.99875915] b =  [0.01455085] MSE =  7.893349367971875\n",
      "w =  [0.99875925] b =  [0.01454409] MSE =  7.893348911839004\n",
      "w =  [0.99875935] b =  [0.01453734] MSE =  7.893348455751867\n",
      "w =  [0.99875945] b =  [0.01453059] MSE =  7.893347999710456\n",
      "w =  [0.99875955] b =  [0.01452384] MSE =  7.893347543714768\n",
      "w =  [0.99875966] b =  [0.01451708] MSE =  7.8933470877648\n",
      "w =  [0.99875976] b =  [0.01451033] MSE =  7.8933466318605445\n",
      "w =  [0.99875986] b =  [0.01450358] MSE =  7.893346176001996\n",
      "w =  [0.99875996] b =  [0.01449683] MSE =  7.893345720189157\n",
      "w =  [0.99876006] b =  [0.01449008] MSE =  7.893345264422013\n",
      "w =  [0.99876016] b =  [0.01448333] MSE =  7.893344808700564\n",
      "w =  [0.99876026] b =  [0.01447658] MSE =  7.893344353024809\n",
      "w =  [0.99876036] b =  [0.01446983] MSE =  7.893343897394737\n",
      "w =  [0.99876046] b =  [0.01446308] MSE =  7.893343441810349\n",
      "w =  [0.99876057] b =  [0.01445634] MSE =  7.893342986271636\n",
      "w =  [0.99876067] b =  [0.01444959] MSE =  7.893342530778596\n",
      "w =  [0.99876077] b =  [0.01444284] MSE =  7.893342075331225\n",
      "w =  [0.99876087] b =  [0.01443609] MSE =  7.893341619929515\n",
      "w =  [0.99876097] b =  [0.01442934] MSE =  7.893341164573465\n",
      "w =  [0.99876107] b =  [0.0144226] MSE =  7.893340709263069\n",
      "w =  [0.99876117] b =  [0.01441585] MSE =  7.89334025399832\n",
      "w =  [0.99876127] b =  [0.0144091] MSE =  7.89333979877922\n",
      "w =  [0.99876137] b =  [0.01440236] MSE =  7.893339343605759\n",
      "w =  [0.99876148] b =  [0.01439561] MSE =  7.89333888847793\n",
      "w =  [0.99876158] b =  [0.01438887] MSE =  7.893338433395736\n",
      "w =  [0.99876168] b =  [0.01438212] MSE =  7.893337978359167\n",
      "w =  [0.99876178] b =  [0.01437538] MSE =  7.89333752336822\n",
      "w =  [0.99876188] b =  [0.01436863] MSE =  7.893337068422892\n",
      "w =  [0.99876198] b =  [0.01436189] MSE =  7.893336613523178\n",
      "w =  [0.99876208] b =  [0.01435515] MSE =  7.893336158669069\n",
      "w =  [0.99876218] b =  [0.0143484] MSE =  7.893335703860565\n",
      "w =  [0.99876228] b =  [0.01434166] MSE =  7.893335249097659\n",
      "w =  [0.99876239] b =  [0.01433492] MSE =  7.89333479438035\n",
      "w =  [0.99876249] b =  [0.01432818] MSE =  7.893334339708628\n",
      "w =  [0.99876259] b =  [0.01432143] MSE =  7.893333885082493\n",
      "w =  [0.99876269] b =  [0.01431469] MSE =  7.893333430501937\n",
      "w =  [0.99876279] b =  [0.01430795] MSE =  7.89333297596696\n",
      "w =  [0.99876289] b =  [0.01430121] MSE =  7.8933325214775545\n",
      "w =  [0.99876299] b =  [0.01429447] MSE =  7.893332067033715\n",
      "w =  [0.99876309] b =  [0.01428773] MSE =  7.893331612635438\n",
      "w =  [0.99876319] b =  [0.01428099] MSE =  7.893331158282721\n",
      "w =  [0.9987633] b =  [0.01427425] MSE =  7.893330703975554\n",
      "w =  [0.9987634] b =  [0.01426751] MSE =  7.893330249713937\n",
      "w =  [0.9987635] b =  [0.01426077] MSE =  7.8933297954978645\n",
      "w =  [0.9987636] b =  [0.01425403] MSE =  7.893329341327332\n",
      "w =  [0.9987637] b =  [0.0142473] MSE =  7.893328887202337\n",
      "w =  [0.9987638] b =  [0.01424056] MSE =  7.893328433122871\n",
      "w =  [0.9987639] b =  [0.01423382] MSE =  7.8933279790889275\n",
      "w =  [0.998764] b =  [0.01422708] MSE =  7.893327525100508\n",
      "w =  [0.9987641] b =  [0.01422035] MSE =  7.893327071157605\n",
      "w =  [0.9987642] b =  [0.01421361] MSE =  7.893326617260216\n",
      "w =  [0.9987643] b =  [0.01420687] MSE =  7.893326163408333\n",
      "w =  [0.99876441] b =  [0.01420014] MSE =  7.893325709601956\n",
      "w =  [0.99876451] b =  [0.0141934] MSE =  7.8933252558410745\n",
      "w =  [0.99876461] b =  [0.01418667] MSE =  7.893324802125688\n",
      "w =  [0.99876471] b =  [0.01417993] MSE =  7.893324348455792\n",
      "w =  [0.99876481] b =  [0.0141732] MSE =  7.89332389483138\n",
      "w =  [0.99876491] b =  [0.01416646] MSE =  7.893323441252449\n",
      "w =  [0.99876501] b =  [0.01415973] MSE =  7.893322987718992\n",
      "w =  [0.99876511] b =  [0.014153] MSE =  7.893322534231008\n",
      "w =  [0.99876521] b =  [0.01414626] MSE =  7.893322080788491\n",
      "w =  [0.99876531] b =  [0.01413953] MSE =  7.893321627391436\n",
      "w =  [0.99876542] b =  [0.0141328] MSE =  7.893321174039837\n",
      "w =  [0.99876552] b =  [0.01412607] MSE =  7.893320720733692\n",
      "w =  [0.99876562] b =  [0.01411934] MSE =  7.893320267472996\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-4f173bb3a193>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrors\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# create vectors of the appropriate shape of training data values\n",
    "x = np.array(df_train['x']).reshape(-1,1)\n",
    "y = np.array(df_train['y']).reshape(-1,1)\n",
    "\n",
    "# key parameters\n",
    "n = 700\n",
    "gamma = 0.00000001\n",
    "epochs = 10000000\n",
    "\n",
    "# initialise our weight and bias vectors with zeros\n",
    "w = np.zeros((n,1))\n",
    "b = np.zeros((n,1))\n",
    "\n",
    "for i in range(0,epochs):\n",
    "    y_predicted = w * x + b\n",
    "        \n",
    "    errors = y_predicted - y\n",
    "    MSE = np.sum(errors**2) / n\n",
    "\n",
    "    w = w - gamma * 2 / n * np.sum(errors * x)\n",
    "    b = b - gamma * 2 / n * np.sum(errors)\n",
    "    \n",
    "    if (i % 10000 == 0):\n",
    "        print(\"w = \", w[0], \"b = \", b[0], \"MSE = \", MSE)\n",
    "\n",
    "print(\"w = \", w[0], \"b = \", b[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w =  [0.9988519] b =  [0.008363]\n"
     ]
    }
   ],
   "source": [
    "print(\"w = \", w[0], \"b = \", b[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
